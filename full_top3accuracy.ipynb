{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cfb44ab-8211-423a-8d55-7257e536781a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"siamese_triplet_model_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"siamese_triplet_model_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ functional_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)       │ ?                      │    <span style=\"color: #00af00; text-decoration-color: #00af00\">40,565,632</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ functional_3 (\u001b[38;5;33mFunctional\u001b[0m)       │ ?                      │    \u001b[38;5;34m40,565,632\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">40,565,632</span> (154.75 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m40,565,632\u001b[0m (154.75 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">25,907,712</span> (98.83 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m25,907,712\u001b[0m (98.83 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,657,920</span> (55.92 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m14,657,920\u001b[0m (55.92 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/tensorflow_env/lib/python3.11/site-packages/keras/src/saving/saving_lib.py:396: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 70 variables. \n",
      "  trackable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "from triplet_dataset_loader import *\n",
    "from TL_class import SiameseTripletModel\n",
    "from TL_siamese_network import generate_siamese_triplet_network\n",
    "from top_accuracy_functions import *\n",
    "\n",
    "database_path = \"../databases/my-fruit-recognition-small\"\n",
    "base_test_folder = \"../databases/my-fruit-recognition-small\"\n",
    "split_ratio = (0.8, 0.1, 0.1)\n",
    "image_size = (100,100)\n",
    "batch_size = 32\n",
    "\n",
    "auto = tf.data.AUTOTUNE\n",
    "\n",
    "learning_rate = 0.0001\n",
    "steps_per_epoch = 50\n",
    "validation_steps = 10\n",
    "epochs = 5\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = create_dataset(database_path, split_ratio, image_size, batch_size)\n",
    "\n",
    "triplet_siamese_network = generate_siamese_triplet_network(image_size)\n",
    "\n",
    "triplet_siamese_model = SiameseTripletModel(triplet_siamese_network)\n",
    "\n",
    "triplet_siamese_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate))\n",
    "triplet_siamese_model.summary()\n",
    "\n",
    "triplet_siamese_model.load_weights(\"../5epoch_model.weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb87584f-ccb7-4f7a-a40a-881fea0440d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = triplet_siamese_network.get_layer(\"Embedding\")\n",
    "\n",
    "# Check if the embedding model is correctly extracted\n",
    "# print(embedding_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f88292dc-3e2d-4519-8ad6-c135bf823a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "Class names: ['Apple/Apple B', 'Apple/Apple A', 'Pear', 'Mango', 'Banana', 'Orange', 'Peach']\n",
      "Number of First Embeddings: 7\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty dictionary to store reference image embeddings with labels\n",
    "reference_embeddings_dict = {}\n",
    "label_names = []\n",
    "\n",
    "# Loop through each subfolder in the base_test_folder\n",
    "for root, dirs, files in os.walk(base_test_folder):\n",
    "  # Skip hidden folders (\".DS_Store\")\n",
    "  files = [f for f in files if f != \".DS_Store\"]\n",
    "\n",
    "  # Check if there are files in the subfolder\n",
    "  if files:\n",
    "    files_sorted = sorted(files)\n",
    "    reference_image_path = os.path.join(root, files_sorted[0])\n",
    "\n",
    "    # Get relative path from base_test_folder\n",
    "    label_name = os.path.relpath(root, base_test_folder)\n",
    "    label_names.append(label_name)\n",
    "\n",
    "    mapF = MapFunction(image_size)\n",
    "    preprocessed_reference_image = mapF.decode_and_resize(reference_image_path)\n",
    "    reference_image_tensor = tf.expand_dims(preprocessed_reference_image, axis=0)\n",
    "    embedding = embedding_model.predict(reference_image_tensor)\n",
    "\n",
    "    # Convert the embedding to a hashable type (e.g., tuple or string)\n",
    "    hashable_embedding = tuple(embedding.flatten())  # Convert to a tuple\n",
    "\n",
    "    # Store the hashable embedding and relative path label in the dictionary\n",
    "    reference_embeddings_dict[hashable_embedding] = label_name\n",
    "\n",
    "print(\"Class names:\", label_names)\n",
    "print(\"Number of Reference Embeddings:\", len(reference_embeddings_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8ab136-6f99-4c48-8755-eada1798b033",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(list(list(first_embeddings_dict.keys())[0])))\n",
    "print(first_embeddings_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd252d28-ea7c-49a4-9317-2c626d22aca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty dictionary to store test embeddings with labels\n",
    "test_embeddings_dict = {}\n",
    "\n",
    "# Generate embeddings for the test images and pair them with labels\n",
    "for image_path, label in test_dataset.items():\n",
    "    # Preprocess the image using the MapFunction\n",
    "    preprocessed_image = mapF.decode_and_resize(image_path)\n",
    "\n",
    "    # Convert the preprocessed image to a tensor and add batch dimension\n",
    "    image_tensor = tf.expand_dims(preprocessed_image, axis=0)\n",
    "\n",
    "    # Generate embedding for the image using the embedding model\n",
    "    embedding = embedding_model.predict(image_tensor)\n",
    "\n",
    "    # Convert embedding numpy array to tuple for hashable key\n",
    "    embedding_tuple = tuple(embedding.flatten())\n",
    "\n",
    "    # Store the embedding tuple and label directly in the dictionary\n",
    "    test_embeddings_dict[embedding_tuple] = label\n",
    "\n",
    "# Print the number of test embeddings generated\n",
    "print(\"Number of Test Embeddings:\", len(test_embeddings_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db54d40e-3223-45b9-9b89-d29b20ca46f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_embeddings_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f592905-3789-4033-b367-d28a88a59e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_top3_accuracy(test_data, reference_data):\n",
    "    total_tests = len(test_data)\n",
    "    correct_top3_count = 0\n",
    "\n",
    "    for test_embedding, test_label in test_data.items():\n",
    "        test_embedding_np = np.array(test_embedding)  # Convert tuple back to numpy array\n",
    "        closest_3_embeddings = top3_accuracy(test_embedding_np, list(reference_data.keys()))\n",
    "\n",
    "        closest_3_labels = [reference_data[embedding] for embedding in closest_3_embeddings]\n",
    "\n",
    "        # print(test_label)\n",
    "        # for label in closest_3_labels:\n",
    "        #     print(label)\n",
    "        # print(correct_top3_count)\n",
    "        if test_label in closest_3_labels:\n",
    "            correct_top3_count += 1\n",
    "        # print(correct_top3_count)\n",
    "\n",
    "    top3_accuracy_value = correct_top3_count / total_tests * 100\n",
    "    return top3_accuracy_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a48156-0c21-4267-ac9c-c39c5d526b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "top3_acc = calculate_top3_accuracy(test_embeddings_dict, first_embeddings_dict)\n",
    "print(f\"Top-3 Accuracy: {top3_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb6a74b-211e-4b3d-9929-1ec19877ee90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
