{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d79680a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50411823",
   "metadata": {},
   "source": [
    "# Define parametes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17fc50d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# model input image size\n",
    "IMAGE_SIZE = (100,100)\n",
    "\n",
    "# batch size and the buffer size\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "BUFFER_SIZE = BATCH_SIZE * 2\n",
    "\n",
    "# define autotune\n",
    "AUTO = tf.data.AUTOTUNE\n",
    "\n",
    "# define the training parameters\n",
    "LEARNING_RATE = 0.0001\n",
    "STEPS_PER_EPOCH = 20\n",
    "VALIDATION_STEPS = 10\n",
    "EPOCHS = 5\n",
    "\n",
    "# define the path to save the model\n",
    "OUTPUT_PATH = \"output\"\n",
    "MODEL_PATH = os.path.join(OUTPUT_PATH, \"siamese_network\")\n",
    "OUTPUT_IMAGE_PATH = os.path.join(OUTPUT_PATH, \"output_image.png\")\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 128\n",
    "margin = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5425cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MapFunction():\n",
    "\tdef __init__(self, imageSize):\n",
    "\t\t# define the image width and height\n",
    "\t\tself.imageSize = imageSize\n",
    "\tdef decode_and_resize(self, imagePath):\n",
    "\t\t# read and decode the image path\n",
    "\t\timage = tf.io.read_file(imagePath)\n",
    "\t\timage = tf.image.decode_jpeg(image, channels=3)\n",
    "\t\t# convert the image data type from uint8 to float32 and then resize\n",
    "\t\t# the image to the set image size\n",
    "\t\timage = tf.image.convert_image_dtype(image, dtype=tf.float32)\n",
    "\t\timage = tf.image.resize(image, self.imageSize)\n",
    "\t\t# return the image\n",
    "\t\treturn image\n",
    "\tdef __call__(self, pair, label):\n",
    "\t\tpositive, negative=pair\n",
    "\t\tpositive = self.decode_and_resize(positive)\n",
    "\t\tnegative = self.decode_and_resize(negative)\n",
    "\t\treturn ( positive, negative), label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74bce71",
   "metadata": {},
   "source": [
    "# PairGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7790613",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairGenerator:\n",
    "    def __init__(self, datasetPath):\n",
    "        self.fruitNames = list()  # path to dir with fruits\n",
    "        for folderName in os.listdir(datasetPath):\n",
    "            absoluteFolderName = os.path.join(datasetPath, folderName)\n",
    "            numImages = len(os.listdir(absoluteFolderName))\n",
    "            if numImages > 1:\n",
    "                self.fruitNames.append(absoluteFolderName)\n",
    "        self.allFruit = self.generate_all_fruit_dict()\n",
    "    def generate_all_fruit_dict(self):\n",
    "        allFruit = dict()\n",
    "        \n",
    "        for fruitName in self.fruitNames:\n",
    "            imageNames = os.listdir(fruitName) # all names of photo one fruit\n",
    "            fruitPhotos = [\n",
    "                os.path.join(fruitName, imageName) for imageName in imageNames\n",
    "            ]\n",
    "            allFruit[fruitName] = fruitPhotos\n",
    "        return allFruit #all path photo in dict\n",
    "    def get_next_element(self):\n",
    "        i=0\n",
    "        while True:\n",
    "            i=i+1\n",
    "                        \n",
    "            \n",
    "            imageNames = random.choice(self.fruitNames)\n",
    "            temporaryNames = self.fruitNames.copy()\n",
    "            temporaryNames.remove(imageNames)\n",
    "            negativeNames = random.choice(temporaryNames)\n",
    "\n",
    "            imagePhoto = random.choice(self.allFruit[imageNames])\n",
    "            positivePhoto = random.choice(self.allFruit[imageNames])\n",
    "            negativePhoto = random.choice(self.allFruit[negativeNames])\n",
    "\n",
    "            yield ((imagePhoto, positivePhoto), 1) \n",
    "            yield ((imagePhoto, negativePhoto), 0)\n",
    "            \n",
    "\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901909fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "567bed5b",
   "metadata": {},
   "source": [
    "# Creating model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d026ca93",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = tf.keras.layers.Input(shape=(100, 100, 3))\n",
    "x = tf.keras.layers.BatchNormalization()(input)\n",
    "x = tf.keras.layers.Conv2D(4, (5, 5), activation=\"tanh\")(x)\n",
    "x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = tf.keras.layers.Conv2D(16, (5, 5), activation=\"tanh\")(x)\n",
    "x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = tf.keras.layers.Dense(100, activation=\"tanh\")(x)\n",
    "embedding_network = tf.keras.Model(input, x)\n",
    "\n",
    "input_1 = tf.keras.layers.Input((100, 100, 3))\n",
    "input_2 = tf.keras.layers.Input((100, 100, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4955de",
   "metadata": {},
   "source": [
    "# Calculate Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "350ad865",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def euclidean_distance(vects):\n",
    "    \"\"\"Find the Euclidean distance between two vectors.\n",
    "\n",
    "    Arguments:\n",
    "        vects: List containing two tensors of same length.\n",
    "\n",
    "    Returns:\n",
    "        Tensor containing euclidean distance\n",
    "        (as floating point value) between vectors.\n",
    "    \"\"\"\n",
    "\n",
    "    x, y = vects\n",
    "    sum_square = tf.keras.backend.sum(tf.keras.backend.square(x - y), axis=1, keepdims=True)\n",
    "    return tf.keras.backend.sqrt(tf.keras.backend.maximum(sum_square, tf.keras.backend.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "691bc1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tower_1 = embedding_network(input_1)\n",
    "tower_2 = embedding_network(input_2)\n",
    "\n",
    "merge_layer = tf.keras.layers.Lambda(euclidean_distance, output_shape=(1,))(\n",
    "    [tower_1, tower_2]\n",
    ")\n",
    "normal_layer = tf.keras.layers.BatchNormalization()(merge_layer)\n",
    "output_layer = tf.keras.layers.Dense(1, activation=\"sigmoid\")(normal_layer)\n",
    "siamese = tf.keras.Model(inputs=[input_1, input_2], outputs=output_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239b402a",
   "metadata": {},
   "source": [
    "# Define the contrastive loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3eceb65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(margin=1):\n",
    "    \"\"\"Provides 'contrastive_loss' an enclosing scope with variable 'margin'.\n",
    "\n",
    "    Arguments:\n",
    "        margin: Integer, defines the baseline for distance for which pairs\n",
    "                should be classified as dissimilar. - (default is 1).\n",
    "\n",
    "    Returns:\n",
    "        'contrastive_loss' function with data ('margin') attached.\n",
    "    \"\"\"\n",
    "\n",
    "    def contrastive_loss(y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "\n",
    "\n",
    "        square_pred = tf.keras.backend.square(y_pred)\n",
    "        margin_square = tf.keras.backend.square(tf.keras.backend.maximum(margin - y_pred, 0))\n",
    "        return tf.keras.backend.mean((1 - y_true) * square_pred + (y_true) * margin_square)\n",
    "\n",
    "\n",
    "    return contrastive_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2bda4a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_2       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ functional_1        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">807,408</span> │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │                   │            │ input_layer_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ functional_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│                     │                   │            │ functional_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span> │ lambda[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span> │ batch_normalizat… │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m100\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │ \u001b[38;5;34m3\u001b[0m)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_2       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m100\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │ \u001b[38;5;34m3\u001b[0m)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ functional_1        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)       │    \u001b[38;5;34m807,408\u001b[0m │ input_layer_1[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mFunctional\u001b[0m)        │                   │            │ input_layer_2[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda (\u001b[38;5;33mLambda\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ functional_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│                     │                   │            │ functional_1[\u001b[38;5;34m1\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m4\u001b[0m │ lambda[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m2\u001b[0m │ batch_normalizat… │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">807,414</span> (3.08 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m807,414\u001b[0m (3.08 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">791,918</span> (3.02 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m791,918\u001b[0m (3.02 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">15,496</span> (60.53 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m15,496\u001b[0m (60.53 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "siamese.compile(loss=loss(margin=margin), optimizer=\"RMSprop\", metrics=[\"accuracy\"])\n",
    "siamese.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0692ef1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a6aede04",
   "metadata": {},
   "source": [
    "TEST\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efde6cf1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9c69427",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "\n",
    "class MapFunction():\n",
    "    def __init__(self, imageSize):\n",
    "        self.imageSize = imageSize\n",
    "\n",
    "    def decode_and_resize(self, imagePath):\n",
    "        image = tf.io.read_file(imagePath)\n",
    "        image = tf.image.decode_jpeg(image, channels=3)\n",
    "        image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n",
    "        image = tf.image.resize(image, self.imageSize)\n",
    "        return image\n",
    "\n",
    "    def __call__(self, pair, label):\n",
    "        positive, negative = pair\n",
    "        positive = self.decode_and_resize(positive)\n",
    "        negative = self.decode_and_resize(negative)\n",
    "        return (positive, negative), label\n",
    "\n",
    "\n",
    "import os\n",
    "import random\n",
    "\n",
    "class PairGenerator1:\n",
    "    def __init__(self, datasetPath, split_ratio=(0.7, 0.2, 0.1)):\n",
    "        self.datasetPath = datasetPath\n",
    "        self.split_ratio = split_ratio\n",
    "        self.label_names = self._get_label_names()\n",
    "        self.label_images = self._generate_label_images_dict()\n",
    "        self.train_images, self.val_images, self.test_images = self._split_label_images()\n",
    "\n",
    "    def _get_label_names(self):\n",
    "        label_names = []\n",
    "        for folder_name in os.listdir(self.datasetPath):\n",
    "            folder_path = os.path.join(self.datasetPath, folder_name)\n",
    "            if os.path.isdir(folder_path):\n",
    "                subfolders = [os.path.join(folder_name, subfolder) for subfolder in os.listdir(folder_path) if os.path.isdir(os.path.join(folder_path, subfolder))]\n",
    "                if subfolders:\n",
    "                    label_names.extend(subfolders)\n",
    "                else:\n",
    "                    label_names.append(folder_name)\n",
    "        return label_names\n",
    "\n",
    "    def _generate_label_images_dict(self):\n",
    "        label_images = {}\n",
    "        for label_name in self.label_names:\n",
    "            label_path = os.path.join(self.datasetPath, label_name)\n",
    "            if os.path.isdir(label_path):\n",
    "                image_files = [os.path.join(label_path, imageName) for imageName in os.listdir(label_path) if os.path.isfile(os.path.join(label_path, imageName))]\n",
    "            else:\n",
    "                image_files = [os.path.join(self.datasetPath, label_name, imageName) for imageName in os.listdir(os.path.join(self.datasetPath, label_name)) if os.path.isfile(os.path.join(self.datasetPath, label_name, imageName))]\n",
    "            label_images[label_name] = image_files\n",
    "        return label_images\n",
    "\n",
    "    def _split_label_images(self):\n",
    "        train_images = []\n",
    "        val_images = []\n",
    "        test_images = []\n",
    "\n",
    "        for label, images in self.label_images.items():\n",
    "            random.shuffle(images)\n",
    "            num_train = int(len(images) * self.split_ratio[0])\n",
    "            num_val = int(len(images) * self.split_ratio[1])\n",
    "\n",
    "            train_images.extend(images[:num_train])\n",
    "            val_images.extend(images[num_train:num_train + num_val])\n",
    "            test_images.extend(images[num_train + num_val:])\n",
    "\n",
    "        return train_images, val_images, test_images\n",
    "\n",
    "    def _get_pair(self, image_set):\n",
    "        while True:\n",
    "            positive_image = random.choice(image_set)\n",
    "            negative_image = random.choice(image_set)\n",
    "\n",
    "            label_positive = os.path.dirname(positive_image)\n",
    "            label_negative = os.path.dirname(negative_image)\n",
    "\n",
    "            if label_positive == label_negative:\n",
    "                yield (positive_image, negative_image), 1\n",
    "            else:\n",
    "                yield (positive_image, negative_image), 0\n",
    "\n",
    "    def get_train_element(self):\n",
    "        return self._get_pair(self.train_images)\n",
    "\n",
    "    def get_val_element(self):\n",
    "        return self._get_pair(self.val_images)\n",
    "\n",
    "    def get_test_element(self):\n",
    "        return self._get_pair(self.test_images)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_dataset(path, img_size, batch_size):\n",
    "    pair_generator = PairGenerator1(path)\n",
    "    image_processor = MapFunction(img_size)\n",
    "\n",
    "    train_dataset = tf.data.Dataset.from_generator(pair_generator.get_train_element,\n",
    "                                                   output_signature=((tf.TensorSpec(shape=(), dtype=tf.string),\n",
    "                                                                      tf.TensorSpec(shape=(), dtype=tf.string)),\n",
    "                                                                     tf.TensorSpec(shape=(), dtype=tf.int32)))\n",
    "\n",
    "    val_dataset = tf.data.Dataset.from_generator(pair_generator.get_val_element,\n",
    "                                                 output_signature=((tf.TensorSpec(shape=(), dtype=tf.string),\n",
    "                                                                    tf.TensorSpec(shape=(), dtype=tf.string)),\n",
    "                                                                   tf.TensorSpec(shape=(), dtype=tf.int32)))\n",
    "\n",
    "    test_dataset = tf.data.Dataset.from_generator(pair_generator.get_test_element,\n",
    "                                                  output_signature=((tf.TensorSpec(shape=(), dtype=tf.string),\n",
    "                                                                     tf.TensorSpec(shape=(), dtype=tf.string)),\n",
    "                                                                    tf.TensorSpec(shape=(), dtype=tf.int32)))\n",
    "\n",
    "    train_dataset = train_dataset.map(image_processor).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    val_dataset = val_dataset.map(image_processor).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    test_dataset = test_dataset.map(image_processor).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50718218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_path=\"/kaggle/input/fruit-recognition\"\n",
    "dataset_path = \"../Training\"\n",
    "# dataset_path=\"C:/Users/tokar/Downloads/archive\"\n",
    "dataset_path=r\"../databases/fruit-recognition\"\n",
    "\n",
    "train_data, val_data, test_data = create_dataset(dataset_path, (100, 100), 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "17c1cc7d-6f8e-4f9c-8e59-25506191dc40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Apple/Apple E', 'Apple/Apple B', 'Apple/Apple C', 'Apple/Apple D', 'Apple/Apple A', 'Apple/Apple F', 'Apple/Total Number of Apples', 'Banana']\n"
     ]
    }
   ],
   "source": [
    "label_names = []\n",
    "for folder_name  in os.listdir(dataset_path):\n",
    "    folder_path = os.path.join(dataset_path, folder_name)\n",
    "    if os.path.isdir(folder_path):\n",
    "        subfolders = [os.path.join(folder_name, subfolder) for subfolder in os.listdir(folder_path) if os.path.isdir(os.path.join(folder_path, subfolder))]\n",
    "        if subfolders:\n",
    "            label_names.extend(subfolders)\n",
    "        else:\n",
    "            label_names.append(folder_name)\n",
    "\n",
    "            \n",
    "print(label_names)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "08a8c52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_images = {}\n",
    "for label_name in label_names:\n",
    "    image_dir = os.path.join(dataset_path, label_name)\n",
    "    image_files = []\n",
    "    for imageName in os.listdir(image_dir):\n",
    "        imagePath = os.path.join(image_dir, imageName)\n",
    "        if os.path.isfile(imagePath):\n",
    "            image_files.append(imagePath)\n",
    "    label_images[label_name] = image_files# print(label_images)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b6da604e-1a13-49cd-9150-7524941cb0de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple/Apple E\n",
      "Apple/Apple B\n",
      "Apple/Apple C\n",
      "Apple/Apple D\n",
      "Apple/Apple A\n",
      "Apple/Apple F\n",
      "Apple/Total Number of Apples\n",
      "Banana\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5024"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# def show_image_pair(pair):\n",
    "#     plt.figure(figsize=(8, 4))\n",
    "#     for i, image in enumerate(pair):\n",
    "#         plt.subplot(1, 2, i + 1)\n",
    "#         plt.imshow(image.numpy())\n",
    "#         plt.axis(\"off\")\n",
    "#     plt.show()\n",
    "\n",
    "# # Przykładowe wyświetlenie par obrazów\n",
    "# for image_pair, label in train_data.take(3):  # Weź pierwsze 3 pary z danych treningowych\n",
    "#     show_image_pair(image_pair)\n",
    "#     print(\"Label:\", label.numpy())\n",
    "\n",
    "for key, val in label_images.items():\n",
    "    print(key)\n",
    "len(label_images[\"Apple/Total Number of Apples\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd850f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f7dc7231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "20/20 [==============================] - 7s 352ms/step - loss: 0.2022 - accuracy: 0.7234 - val_loss: 0.2371 - val_accuracy: 0.5969\n",
      "Epoch 2/10\n",
      "20/20 [==============================] - 5s 262ms/step - loss: 0.1911 - accuracy: 0.7469 - val_loss: 0.2299 - val_accuracy: 0.5750\n",
      "Epoch 3/10\n",
      "20/20 [==============================] - 5s 250ms/step - loss: 0.1862 - accuracy: 0.7703 - val_loss: 0.1989 - val_accuracy: 0.7031\n",
      "Epoch 4/10\n",
      "20/20 [==============================] - 5s 253ms/step - loss: 0.1778 - accuracy: 0.8078 - val_loss: 0.2000 - val_accuracy: 0.7125\n",
      "Epoch 5/10\n",
      "20/20 [==============================] - 4s 231ms/step - loss: 0.1742 - accuracy: 0.8266 - val_loss: 0.2002 - val_accuracy: 0.7094\n",
      "Epoch 6/10\n",
      "20/20 [==============================] - 5s 233ms/step - loss: 0.1669 - accuracy: 0.8422 - val_loss: 0.1800 - val_accuracy: 0.7781\n",
      "Epoch 7/10\n",
      "20/20 [==============================] - 4s 226ms/step - loss: 0.1691 - accuracy: 0.8656 - val_loss: 0.1686 - val_accuracy: 0.8344\n",
      "Epoch 8/10\n",
      "20/20 [==============================] - 4s 224ms/step - loss: 0.1594 - accuracy: 0.8859 - val_loss: 0.1449 - val_accuracy: 0.9062\n",
      "Epoch 9/10\n",
      "20/20 [==============================] - 4s 222ms/step - loss: 0.1570 - accuracy: 0.9078 - val_loss: 0.1558 - val_accuracy: 0.9000\n",
      "Epoch 10/10\n",
      "20/20 [==============================] - 4s 216ms/step - loss: 0.1521 - accuracy: 0.9016 - val_loss: 0.1414 - val_accuracy: 0.9281\n"
     ]
    }
   ],
   "source": [
    "history = siamese.fit(\n",
    "    train_data,\n",
    "    validation_data=val_data,\n",
    "    steps_per_epoch=STEPS_PER_EPOCH,\n",
    "    validation_steps=VALIDATION_STEPS,\n",
    "    epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c2dd49e8",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] Unable to create file (unable to open file: name = '/kaggle/working/fruitrecognition.weights.h5', errno = 2, error message = 'No such file or directory', flags = 13, o_flags = 602)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m siamese\u001b[38;5;241m.\u001b[39msave_weights(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/kaggle/working/fruitrecognition.weights.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m, overwrite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/tensorflow_env/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:123\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/tensorflow_env/lib/python3.11/site-packages/h5py/_hl/files.py:562\u001b[0m, in \u001b[0;36mFile.__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[1;32m    553\u001b[0m     fapl \u001b[38;5;241m=\u001b[39m make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,\n\u001b[1;32m    554\u001b[0m                      locking, page_buf_size, min_meta_keep, min_raw_keep,\n\u001b[1;32m    555\u001b[0m                      alignment_threshold\u001b[38;5;241m=\u001b[39malignment_threshold,\n\u001b[1;32m    556\u001b[0m                      alignment_interval\u001b[38;5;241m=\u001b[39malignment_interval,\n\u001b[1;32m    557\u001b[0m                      meta_block_size\u001b[38;5;241m=\u001b[39mmeta_block_size,\n\u001b[1;32m    558\u001b[0m                      \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    559\u001b[0m     fcpl \u001b[38;5;241m=\u001b[39m make_fcpl(track_order\u001b[38;5;241m=\u001b[39mtrack_order, fs_strategy\u001b[38;5;241m=\u001b[39mfs_strategy,\n\u001b[1;32m    560\u001b[0m                      fs_persist\u001b[38;5;241m=\u001b[39mfs_persist, fs_threshold\u001b[38;5;241m=\u001b[39mfs_threshold,\n\u001b[1;32m    561\u001b[0m                      fs_page_size\u001b[38;5;241m=\u001b[39mfs_page_size)\n\u001b[0;32m--> 562\u001b[0m     fid \u001b[38;5;241m=\u001b[39m make_fid(name, mode, userblock_size, fapl, fcpl, swmr\u001b[38;5;241m=\u001b[39mswmr)\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(libver, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    565\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_libver \u001b[38;5;241m=\u001b[39m libver\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/tensorflow_env/lib/python3.11/site-packages/h5py/_hl/files.py:241\u001b[0m, in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    239\u001b[0m     fid \u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mcreate(name, h5f\u001b[38;5;241m.\u001b[39mACC_EXCL, fapl\u001b[38;5;241m=\u001b[39mfapl, fcpl\u001b[38;5;241m=\u001b[39mfcpl)\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 241\u001b[0m     fid \u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mcreate(name, h5f\u001b[38;5;241m.\u001b[39mACC_TRUNC, fapl\u001b[38;5;241m=\u001b[39mfapl, fcpl\u001b[38;5;241m=\u001b[39mfcpl)\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;66;03m# Open in append mode (read/write).\u001b[39;00m\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;66;03m# If that fails, create a new file only if it won't clobber an\u001b[39;00m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;66;03m# existing one (ACC_EXCL)\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5f.pyx:122\u001b[0m, in \u001b[0;36mh5py.h5f.create\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] Unable to create file (unable to open file: name = '/kaggle/working/fruitrecognition.weights.h5', errno = 2, error message = 'No such file or directory', flags = 13, o_flags = 602)"
     ]
    }
   ],
   "source": [
    "siamese.save_weights(\"/kaggle/working/fruitrecognition.weights.h5\", overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144867b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first=tf.keras.Model(inputs=[input_1, input_2], outputs=output_layer)\n",
    "# first.load_weights(\n",
    "#    \"/kaggle/working/360.weights.h5\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6239d110",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ad0b8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 500992,
     "sourceId": 928052,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5857,
     "sourceId": 2609027,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30698,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
